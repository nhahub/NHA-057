{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqlOWNx3883r",
        "outputId": "aca10a27-57ad-472f-e074-45c9efa21ae3"
      },
      "id": "aqlOWNx3883r",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DRIVE = '/content/drive/MyDrive/SignBridge_Data'\n",
        "os.makedirs(f\"{DRIVE}/speech/raw_audio\", exist_ok=True)\n",
        "os.makedirs(f\"{DRIVE}/speech/transcripts\", exist_ok=True)\n",
        "\n",
        "os.makedirs(\"nlp/stt\", exist_ok=True)\n",
        "\n",
        "!pip install openai-whisper librosa ffmpeg-python --quiet\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpebrK_5-TvB",
        "outputId": "e5efb15a-c4ba-438f-cd81-58c3839dd226"
      },
      "id": "kpebrK_5-TvB",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m798.7/803.2 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "print(\"\\nEnvironment setup complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwI1WjpsAwK6",
        "outputId": "2b6c0a6f-f721-4f70-fc3a-f44fca6d22e4"
      },
      "id": "FwI1WjpsAwK6",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "\n",
            "Environment setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import time\n",
        "\n",
        "model_size = \"medium\"\n",
        "start_time = time.time()\n",
        "\n",
        "model = whisper.load_model(model_size)\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "print(f\"Model '{model_size}' loaded in {load_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "print(f\"\\nModel Properties\")\n",
        "print(f\"- Size: {model_size}\")\n",
        "print(f\"- Parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
        "print(f\"- Language detection: Yes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYJujFSgEDmu",
        "outputId": "fa1d5df2-2364-4a37-8f95-41cd6f9fe9a6"
      },
      "id": "wYJujFSgEDmu",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.42G/1.42G [00:44<00:00, 34.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 'medium' loaded in 59.12 seconds\n",
            "\n",
            "Model Properties\n",
            "- Size: medium\n",
            "- Parameters: 762.3M\n",
            "- Language detection: Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sounddevice scipy --quiet\n",
        "# Install librosa if not already installed\n",
        "!pip install librosa --quiet"
      ],
      "metadata": {
        "id": "ZCJld8CJOmmT"
      },
      "id": "ZCJld8CJOmmT",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Audio recording with voice activity detection\n",
        "from IPython.display import Javascript, display, HTML\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def record_with_vad(max_seconds=30, silence_threshold=2):\n",
        "    \"\"\"\n",
        "    Record audio with voice activity detection to stop recording after silence.\n",
        "\n",
        "    Args:\n",
        "        max_seconds: Maximum recording duration in seconds\n",
        "        silence_threshold: Number of seconds of silence to trigger stop\n",
        "    \"\"\"\n",
        "    print(f\"Recording up to {max_seconds} seconds. Will stop after {silence_threshold}s of silence.\")\n",
        "    print(\"Speak clearly and press Start Recording when ready.\")\n",
        "\n",
        "    # Fixed HTML with better contrast - dark text on light background\n",
        "    display(HTML(\"\"\"\n",
        "    <div style=\"background-color:#f8f9fa;padding:12px;border-radius:6px;margin-bottom:12px;border:1px solid #ddd;\">\n",
        "        <p style=\"font-weight:bold;color:#202124;font-size:16px;margin-bottom:8px;\">üì£ Recording Tips:</p>\n",
        "        <ul style=\"color:#202124;font-size:14px;margin-left:20px;\">\n",
        "            <li>Recording will automatically stop after a few seconds of silence</li>\n",
        "            <li>You can also click Stop when you're done speaking</li>\n",
        "            <li>Speak clearly at a normal pace</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "    \"\"\"))\n",
        "\n",
        "    js = Javascript(\"\"\"\n",
        "    async function recordAudio() {\n",
        "        const div = document.createElement('div');\n",
        "        div.style = 'padding: 10px; border: 1px solid #ddd; border-radius: 8px;';\n",
        "\n",
        "        const audio = document.createElement('audio');\n",
        "        const recordButton = document.createElement('button');\n",
        "        recordButton.textContent = 'üéôÔ∏è Start Recording';\n",
        "        recordButton.style = 'background-color:#4CAF50;color:white;border:none;padding:10px 20px;border-radius:4px;font-size:16px;cursor:pointer;';\n",
        "\n",
        "        const statusLabel = document.createElement('p');\n",
        "        statusLabel.textContent = 'Ready to record';\n",
        "\n",
        "        const timerLabel = document.createElement('p');\n",
        "        timerLabel.textContent = '00:00';\n",
        "\n",
        "        const visualizer = document.createElement('canvas');\n",
        "        visualizer.width = 300;\n",
        "        visualizer.height = 60;\n",
        "        visualizer.style = 'width:100%;background-color:#f5f5f5;border-radius:4px;';\n",
        "\n",
        "        div.appendChild(recordButton);\n",
        "        div.appendChild(statusLabel);\n",
        "        div.appendChild(timerLabel);\n",
        "        div.appendChild(visualizer);\n",
        "        div.appendChild(audio);\n",
        "        document.body.appendChild(div);\n",
        "\n",
        "        try {\n",
        "            // Request high-quality audio\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({\n",
        "                audio: {\n",
        "                    echoCancellation: true,\n",
        "                    noiseSuppression: true,\n",
        "                    autoGainControl: true,\n",
        "                    sampleRate: 48000\n",
        "                }\n",
        "            });\n",
        "\n",
        "            // Set up audio processing for voice activity detection\n",
        "            const audioContext = new AudioContext();\n",
        "            const source = audioContext.createMediaStreamSource(stream);\n",
        "            const analyser = audioContext.createAnalyser();\n",
        "            analyser.fftSize = 2048;\n",
        "            source.connect(analyser);\n",
        "\n",
        "            // For visualizer\n",
        "            const canvasCtx = visualizer.getContext('2d');\n",
        "            const bufferLength = analyser.frequencyBinCount;\n",
        "            const dataArray = new Uint8Array(bufferLength);\n",
        "\n",
        "            // Voice activity detection variables\n",
        "            let silenceStart = null;\n",
        "            let isSpeaking = false;\n",
        "            const silenceThreshold = 15; // Threshold for detecting speech vs silence\n",
        "\n",
        "            // Set up recorder\n",
        "            const mediaRecorder = new MediaRecorder(stream, {\n",
        "                mimeType: 'audio/webm;codecs=opus',\n",
        "                audioBitsPerSecond: 128000\n",
        "            });\n",
        "\n",
        "            const chunks = [];\n",
        "            let startTime;\n",
        "            let timerInterval;\n",
        "\n",
        "            mediaRecorder.addEventListener('dataavailable', event => {\n",
        "                chunks.push(event.data);\n",
        "            });\n",
        "\n",
        "            // Function to visualize audio and detect silence\n",
        "            function visualize() {\n",
        "                if (!mediaRecorder || mediaRecorder.state !== 'recording') return;\n",
        "\n",
        "                analyser.getByteFrequencyData(dataArray);\n",
        "\n",
        "                // Calculate volume\n",
        "                let sum = 0;\n",
        "                for(let i = 0; i < bufferLength; i++) {\n",
        "                    sum += dataArray[i];\n",
        "                }\n",
        "                let average = sum / bufferLength;\n",
        "\n",
        "                // Detect speech/silence\n",
        "                if (average > silenceThreshold) {\n",
        "                    isSpeaking = true;\n",
        "                    silenceStart = null;\n",
        "                    statusLabel.textContent = 'Recording: Speech detected';\n",
        "                } else if (isSpeaking) {\n",
        "                    if (!silenceStart) {\n",
        "                        silenceStart = Date.now();\n",
        "                        statusLabel.textContent = 'Recording: Silence detected';\n",
        "                    } else if ((Date.now() - silenceStart) > \"\"\" + str(silence_threshold * 1000) + \"\"\") {\n",
        "                        statusLabel.textContent = 'Recording stopped: Silence detected';\n",
        "                        stop();\n",
        "                        return;\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                // Clear canvas\n",
        "                canvasCtx.fillStyle = '#f5f5f5';\n",
        "                canvasCtx.fillRect(0, 0, visualizer.width, visualizer.height);\n",
        "\n",
        "                // Draw visualization\n",
        "                canvasCtx.lineWidth = 2;\n",
        "                canvasCtx.strokeStyle = isSpeaking ? '#4CAF50' : '#999';\n",
        "                canvasCtx.beginPath();\n",
        "\n",
        "                const sliceWidth = visualizer.width * 1.0 / bufferLength;\n",
        "                let x = 0;\n",
        "\n",
        "                for(let i = 0; i < bufferLength; i++) {\n",
        "                    const v = dataArray[i] / 128.0;\n",
        "                    const y = v * visualizer.height/2;\n",
        "\n",
        "                    if(i === 0) {\n",
        "                        canvasCtx.moveTo(x, y);\n",
        "                    } else {\n",
        "                        canvasCtx.lineTo(x, y);\n",
        "                    }\n",
        "\n",
        "                    x += sliceWidth;\n",
        "                }\n",
        "\n",
        "                canvasCtx.lineTo(visualizer.width, visualizer.height/2);\n",
        "                canvasCtx.stroke();\n",
        "\n",
        "                requestAnimationFrame(visualize);\n",
        "            }\n",
        "\n",
        "            const updateTimer = () => {\n",
        "                const now = Date.now();\n",
        "                const diff = now - startTime;\n",
        "                const seconds = Math.floor(diff / 1000);\n",
        "                const minutes = Math.floor(seconds / 60);\n",
        "                const remainingSeconds = seconds % 60;\n",
        "                timerLabel.textContent = `${minutes.toString().padStart(2, '0')}:${remainingSeconds.toString().padStart(2, '0')}`;\n",
        "\n",
        "                // Check max duration\n",
        "                if (seconds >= \"\"\" + str(max_seconds) + \"\"\") {\n",
        "                    stop();\n",
        "                }\n",
        "            };\n",
        "\n",
        "            const start = () => {\n",
        "                chunks.length = 0;\n",
        "                mediaRecorder.start(100); // Get data every 100ms for smoother updates\n",
        "                startTime = Date.now();\n",
        "                timerInterval = setInterval(updateTimer, 100);\n",
        "                visualize();\n",
        "\n",
        "                recordButton.textContent = '‚èπÔ∏è Stop Recording';\n",
        "                recordButton.style.backgroundColor = '#f44336';\n",
        "                statusLabel.textContent = 'Recording...';\n",
        "                recordButton.onclick = stop;\n",
        "                isSpeaking = false;\n",
        "            };\n",
        "\n",
        "            const stop = () => {\n",
        "                if (mediaRecorder.state === 'inactive') return;\n",
        "\n",
        "                mediaRecorder.stop();\n",
        "                clearInterval(timerInterval);\n",
        "\n",
        "                stream.getTracks().forEach(track => track.stop());\n",
        "                audioContext.close();\n",
        "\n",
        "                recordButton.textContent = '‚úÖ Processing...';\n",
        "                recordButton.disabled = true;\n",
        "                recordButton.style.backgroundColor = '#9e9e9e';\n",
        "            };\n",
        "\n",
        "            recordButton.onclick = start;\n",
        "\n",
        "            const recording = new Promise(resolve => {\n",
        "                mediaRecorder.addEventListener('stop', () => {\n",
        "                    const blob = new Blob(chunks, {'type': 'audio/webm'});\n",
        "                    audio.src = URL.createObjectURL(blob);\n",
        "                    audio.controls = true;\n",
        "                    statusLabel.textContent = 'Recording complete! You can play it below:';\n",
        "\n",
        "                    // Convert to base64\n",
        "                    const reader = new FileReader();\n",
        "                    reader.onload = () => {\n",
        "                        resolve(reader.result.split(',')[1]);\n",
        "                    };\n",
        "                    reader.readAsDataURL(blob);\n",
        "                });\n",
        "            });\n",
        "\n",
        "            return await recording;\n",
        "        } catch (err) {\n",
        "            statusLabel.textContent = 'Error: ' + err.message;\n",
        "            console.error('Recording error:', err);\n",
        "            return null;\n",
        "        }\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "    display(js)\n",
        "    s = output.eval_js('recordAudio()')\n",
        "\n",
        "    if s is None:\n",
        "        print(\"Recording failed or was canceled.\")\n",
        "        return None\n",
        "\n",
        "    # Save as webm first (original format from browser)\n",
        "    binary = b64decode(s)\n",
        "    timestamp = int(time.time())\n",
        "    webm_path = f\"{DRIVE}/speech/raw_audio/recording_{timestamp}.webm\"\n",
        "\n",
        "    with open(webm_path, 'wb') as f:\n",
        "        f.write(binary)\n",
        "\n",
        "    # Convert to WAV using ffmpeg (better for whisper)\n",
        "    print(\"Converting audio to WAV format...\")\n",
        "\n",
        "    wav_path = f\"{DRIVE}/speech/raw_audio/recording_{timestamp}.wav\"\n",
        "    !ffmpeg -i \"{webm_path}\" -ar 16000 -ac 1 -c:a pcm_s16le -hide_banner -loglevel error \"{wav_path}\"\n",
        "\n",
        "    # Remove the webm file\n",
        "    !rm \"{webm_path}\"\n",
        "\n",
        "    print(f\"Recording saved to {wav_path}\")\n",
        "    return wav_path"
      ],
      "metadata": {
        "id": "JLsjHv3LGXrE"
      },
      "id": "JLsjHv3LGXrE",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Audio transcription with improved options\n",
        "import librosa\n",
        "\n",
        "def transcribe_with_options(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribe audio file with optimized settings and proper metrics.\n",
        "\n",
        "    Args:\n",
        "        audio_path: Path to the audio file\n",
        "\n",
        "    Returns:\n",
        "        Transcription result dictionary\n",
        "    \"\"\"\n",
        "    if not os.path.exists(audio_path):\n",
        "        print(f\"Error: Audio file not found at {audio_path}\")\n",
        "        return None\n",
        "\n",
        "    # Verify the audio file and get its duration\n",
        "    try:\n",
        "        # Load audio and get duration\n",
        "        y, sr = librosa.load(audio_path, sr=16000)\n",
        "        audio_duration = librosa.get_duration(y=y, sr=sr)\n",
        "        print(f\"Audio loaded successfully. Duration: {audio_duration:.2f} seconds\")\n",
        "\n",
        "        # If audio is too short or appears to be silent, there might be an issue\n",
        "        if audio_duration < 0.1:\n",
        "            print(\"Warning: Audio file is extremely short or might be empty.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not verify audio with librosa: {e}\")\n",
        "        audio_duration = None\n",
        "\n",
        "    print(\"\\nTranscribing with Whisper...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Perform transcription with specific options for better results\n",
        "    result = model.transcribe(\n",
        "        audio_path,\n",
        "        language=\"en\",  # Force English language detection\n",
        "        fp16=False      # Use full precision (may help with quality)\n",
        "    )\n",
        "\n",
        "    # Calculate metrics\n",
        "    transcription_time = time.time() - start_time\n",
        "\n",
        "    # Use librosa's duration if whisper's duration is invalid\n",
        "    if result.get(\"duration\", 0) <= 0:\n",
        "        result[\"duration\"] = audio_duration if audio_duration is not None else 0\n",
        "\n",
        "    # Calculate RTF only if we have valid duration\n",
        "    rtf = transcription_time / result[\"duration\"] if result[\"duration\"] > 0 else 0\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n--- Transcription Results ---\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Time taken: {transcription_time:.2f} seconds\")\n",
        "    print(f\"Audio duration: {result['duration']:.2f} seconds\")\n",
        "    print(f\"Real-time factor (RTF): {rtf:.2f}x\")\n",
        "    print(\"----------------------------\")\n",
        "\n",
        "    # Save transcription to file\n",
        "    transcript_path = f\"{DRIVE}/speech/transcripts/{os.path.splitext(os.path.basename(audio_path))[0]}_transcript.txt\"\n",
        "    with open(transcript_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(result['text'])\n",
        "\n",
        "    print(f\"Transcript saved to {transcript_path}\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "dBBNoqLb2O40"
      },
      "id": "dBBNoqLb2O40",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 5: test regular transcription\n",
        "def test_regular_transcription():\n",
        "  print(\"Testing regular transcription...\")\n",
        "  audio_path = record_with_vad(max_seconds=30, silence_threshold=2)\n",
        "  if audio_path:\n",
        "    transcription_result = transcribe_with_options(audio_path)\n",
        "    return audio_path, transcription_result\n",
        "  return None, None\n",
        "\n",
        "#run the test\n",
        "test_audio_path, test_result = test_regular_transcription()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "W2UUQdCk3O1e",
        "outputId": "bf825ea8-4a8b-4772-fa7d-35514ad972ff"
      },
      "id": "W2UUQdCk3O1e",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing regular transcription...\n",
            "Recording up to 30 seconds. Will stop after 2s of silence.\n",
            "Speak clearly and press Start Recording when ready.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"background-color:#f8f9fa;padding:12px;border-radius:6px;margin-bottom:12px;border:1px solid #ddd;\">\n",
              "        <p style=\"font-weight:bold;color:#202124;font-size:16px;margin-bottom:8px;\">üì£ Recording Tips:</p>\n",
              "        <ul style=\"color:#202124;font-size:14px;margin-left:20px;\">\n",
              "            <li>Recording will automatically stop after a few seconds of silence</li>\n",
              "            <li>You can also click Stop when you're done speaking</li>\n",
              "            <li>Speak clearly at a normal pace</li>\n",
              "        </ul>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function recordAudio() {\n",
              "        const div = document.createElement('div');\n",
              "        div.style = 'padding: 10px; border: 1px solid #ddd; border-radius: 8px;';\n",
              "        \n",
              "        const audio = document.createElement('audio');\n",
              "        const recordButton = document.createElement('button');\n",
              "        recordButton.textContent = 'üéôÔ∏è Start Recording';\n",
              "        recordButton.style = 'background-color:#4CAF50;color:white;border:none;padding:10px 20px;border-radius:4px;font-size:16px;cursor:pointer;';\n",
              "        \n",
              "        const statusLabel = document.createElement('p');\n",
              "        statusLabel.textContent = 'Ready to record';\n",
              "        \n",
              "        const timerLabel = document.createElement('p');\n",
              "        timerLabel.textContent = '00:00';\n",
              "        \n",
              "        const visualizer = document.createElement('canvas');\n",
              "        visualizer.width = 300;\n",
              "        visualizer.height = 60;\n",
              "        visualizer.style = 'width:100%;background-color:#f5f5f5;border-radius:4px;';\n",
              "        \n",
              "        div.appendChild(recordButton);\n",
              "        div.appendChild(statusLabel);\n",
              "        div.appendChild(timerLabel);\n",
              "        div.appendChild(visualizer);\n",
              "        div.appendChild(audio);\n",
              "        document.body.appendChild(div);\n",
              "        \n",
              "        try {\n",
              "            // Request high-quality audio\n",
              "            const stream = await navigator.mediaDevices.getUserMedia({\n",
              "                audio: {\n",
              "                    echoCancellation: true,\n",
              "                    noiseSuppression: true,\n",
              "                    autoGainControl: true,\n",
              "                    sampleRate: 48000\n",
              "                }\n",
              "            });\n",
              "            \n",
              "            // Set up audio processing for voice activity detection\n",
              "            const audioContext = new AudioContext();\n",
              "            const source = audioContext.createMediaStreamSource(stream);\n",
              "            const analyser = audioContext.createAnalyser();\n",
              "            analyser.fftSize = 2048;\n",
              "            source.connect(analyser);\n",
              "            \n",
              "            // For visualizer\n",
              "            const canvasCtx = visualizer.getContext('2d');\n",
              "            const bufferLength = analyser.frequencyBinCount;\n",
              "            const dataArray = new Uint8Array(bufferLength);\n",
              "            \n",
              "            // Voice activity detection variables\n",
              "            let silenceStart = null;\n",
              "            let isSpeaking = false;\n",
              "            const silenceThreshold = 15; // Threshold for detecting speech vs silence\n",
              "            \n",
              "            // Set up recorder\n",
              "            const mediaRecorder = new MediaRecorder(stream, {\n",
              "                mimeType: 'audio/webm;codecs=opus',\n",
              "                audioBitsPerSecond: 128000\n",
              "            });\n",
              "            \n",
              "            const chunks = [];\n",
              "            let startTime;\n",
              "            let timerInterval;\n",
              "            \n",
              "            mediaRecorder.addEventListener('dataavailable', event => {\n",
              "                chunks.push(event.data);\n",
              "            });\n",
              "            \n",
              "            // Function to visualize audio and detect silence\n",
              "            function visualize() {\n",
              "                if (!mediaRecorder || mediaRecorder.state !== 'recording') return;\n",
              "                \n",
              "                analyser.getByteFrequencyData(dataArray);\n",
              "                \n",
              "                // Calculate volume\n",
              "                let sum = 0;\n",
              "                for(let i = 0; i < bufferLength; i++) {\n",
              "                    sum += dataArray[i];\n",
              "                }\n",
              "                let average = sum / bufferLength;\n",
              "                \n",
              "                // Detect speech/silence\n",
              "                if (average > silenceThreshold) {\n",
              "                    isSpeaking = true;\n",
              "                    silenceStart = null;\n",
              "                    statusLabel.textContent = 'Recording: Speech detected';\n",
              "                } else if (isSpeaking) {\n",
              "                    if (!silenceStart) {\n",
              "                        silenceStart = Date.now();\n",
              "                        statusLabel.textContent = 'Recording: Silence detected';\n",
              "                    } else if ((Date.now() - silenceStart) > 2000) {\n",
              "                        statusLabel.textContent = 'Recording stopped: Silence detected';\n",
              "                        stop();\n",
              "                        return;\n",
              "                    }\n",
              "                }\n",
              "                \n",
              "                // Clear canvas\n",
              "                canvasCtx.fillStyle = '#f5f5f5';\n",
              "                canvasCtx.fillRect(0, 0, visualizer.width, visualizer.height);\n",
              "                \n",
              "                // Draw visualization\n",
              "                canvasCtx.lineWidth = 2;\n",
              "                canvasCtx.strokeStyle = isSpeaking ? '#4CAF50' : '#999';\n",
              "                canvasCtx.beginPath();\n",
              "                \n",
              "                const sliceWidth = visualizer.width * 1.0 / bufferLength;\n",
              "                let x = 0;\n",
              "                \n",
              "                for(let i = 0; i < bufferLength; i++) {\n",
              "                    const v = dataArray[i] / 128.0;\n",
              "                    const y = v * visualizer.height/2;\n",
              "                    \n",
              "                    if(i === 0) {\n",
              "                        canvasCtx.moveTo(x, y);\n",
              "                    } else {\n",
              "                        canvasCtx.lineTo(x, y);\n",
              "                    }\n",
              "                    \n",
              "                    x += sliceWidth;\n",
              "                }\n",
              "                \n",
              "                canvasCtx.lineTo(visualizer.width, visualizer.height/2);\n",
              "                canvasCtx.stroke();\n",
              "                \n",
              "                requestAnimationFrame(visualize);\n",
              "            }\n",
              "            \n",
              "            const updateTimer = () => {\n",
              "                const now = Date.now();\n",
              "                const diff = now - startTime;\n",
              "                const seconds = Math.floor(diff / 1000);\n",
              "                const minutes = Math.floor(seconds / 60);\n",
              "                const remainingSeconds = seconds % 60;\n",
              "                timerLabel.textContent = `${minutes.toString().padStart(2, '0')}:${remainingSeconds.toString().padStart(2, '0')}`;\n",
              "                \n",
              "                // Check max duration\n",
              "                if (seconds >= 30) {\n",
              "                    stop();\n",
              "                }\n",
              "            };\n",
              "            \n",
              "            const start = () => {\n",
              "                chunks.length = 0;\n",
              "                mediaRecorder.start(100); // Get data every 100ms for smoother updates\n",
              "                startTime = Date.now();\n",
              "                timerInterval = setInterval(updateTimer, 100);\n",
              "                visualize();\n",
              "                \n",
              "                recordButton.textContent = '‚èπÔ∏è Stop Recording';\n",
              "                recordButton.style.backgroundColor = '#f44336';\n",
              "                statusLabel.textContent = 'Recording...';\n",
              "                recordButton.onclick = stop;\n",
              "                isSpeaking = false;\n",
              "            };\n",
              "            \n",
              "            const stop = () => {\n",
              "                if (mediaRecorder.state === 'inactive') return;\n",
              "                \n",
              "                mediaRecorder.stop();\n",
              "                clearInterval(timerInterval);\n",
              "                \n",
              "                stream.getTracks().forEach(track => track.stop());\n",
              "                audioContext.close();\n",
              "                \n",
              "                recordButton.textContent = '‚úÖ Processing...';\n",
              "                recordButton.disabled = true;\n",
              "                recordButton.style.backgroundColor = '#9e9e9e';\n",
              "            };\n",
              "            \n",
              "            recordButton.onclick = start;\n",
              "            \n",
              "            const recording = new Promise(resolve => {\n",
              "                mediaRecorder.addEventListener('stop', () => {\n",
              "                    const blob = new Blob(chunks, {'type': 'audio/webm'});\n",
              "                    audio.src = URL.createObjectURL(blob);\n",
              "                    audio.controls = true;\n",
              "                    statusLabel.textContent = 'Recording complete! You can play it below:';\n",
              "                    \n",
              "                    // Convert to base64\n",
              "                    const reader = new FileReader();\n",
              "                    reader.onload = () => {\n",
              "                        resolve(reader.result.split(',')[1]);\n",
              "                    };\n",
              "                    reader.readAsDataURL(blob);\n",
              "                });\n",
              "            });\n",
              "            \n",
              "            return await recording;\n",
              "        } catch (err) {\n",
              "            statusLabel.textContent = 'Error: ' + err.message;\n",
              "            console.error('Recording error:', err);\n",
              "            return null;\n",
              "        }\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting audio to WAV format...\n",
            "Recording saved to /content/drive/MyDrive/SignBridge_Data/speech/raw_audio/recording_1761152859.wav\n",
            "Audio loaded successfully. Duration: 3.78 seconds\n",
            "\n",
            "Transcribing with Whisper...\n",
            "\n",
            "--- Transcription Results ---\n",
            "Text:  What's up bro, how are you?\n",
            "Time taken: 0.81 seconds\n",
            "Audio duration: 3.78 seconds\n",
            "Real-time factor (RTF): 0.21x\n",
            "----------------------------\n",
            "Transcript saved to /content/drive/MyDrive/SignBridge_Data/speech/transcripts/recording_1761152859_transcript.txt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}